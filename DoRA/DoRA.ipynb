{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANuER4K4uWKq",
        "outputId": "586fb29b-da80-4f59-8386-8ac2f1d9ba71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Loss: 10.946558952331543\n",
            "Epoch 2/100, Loss: 10.67071533203125\n",
            "Epoch 3/100, Loss: 10.415446281433105\n",
            "Epoch 4/100, Loss: 10.147682189941406\n",
            "Epoch 5/100, Loss: 9.901562690734863\n",
            "Epoch 6/100, Loss: 9.643220901489258\n",
            "Epoch 7/100, Loss: 9.403273582458496\n",
            "Epoch 8/100, Loss: 9.166988372802734\n",
            "Epoch 9/100, Loss: 8.932511329650879\n",
            "Epoch 10/100, Loss: 8.701026916503906\n",
            "Epoch 11/100, Loss: 8.4810152053833\n",
            "Epoch 12/100, Loss: 8.261302947998047\n",
            "Epoch 13/100, Loss: 8.046365737915039\n",
            "Epoch 14/100, Loss: 7.83701229095459\n",
            "Epoch 15/100, Loss: 7.634801864624023\n",
            "Epoch 16/100, Loss: 7.4315185546875\n",
            "Epoch 17/100, Loss: 7.236100673675537\n",
            "Epoch 18/100, Loss: 7.042198657989502\n",
            "Epoch 19/100, Loss: 6.861471176147461\n",
            "Epoch 20/100, Loss: 6.6720757484436035\n",
            "Epoch 21/100, Loss: 6.495471000671387\n",
            "Epoch 22/100, Loss: 6.318413734436035\n",
            "Epoch 23/100, Loss: 6.148136615753174\n",
            "Epoch 24/100, Loss: 5.983376979827881\n",
            "Epoch 25/100, Loss: 5.817665100097656\n",
            "Epoch 26/100, Loss: 5.656281471252441\n",
            "Epoch 27/100, Loss: 5.501224994659424\n",
            "Epoch 28/100, Loss: 5.347464561462402\n",
            "Epoch 29/100, Loss: 5.199982643127441\n",
            "Epoch 30/100, Loss: 5.054170608520508\n",
            "Epoch 31/100, Loss: 4.910162448883057\n",
            "Epoch 32/100, Loss: 4.775045394897461\n",
            "Epoch 33/100, Loss: 4.636500835418701\n",
            "Epoch 34/100, Loss: 4.505670547485352\n",
            "Epoch 35/100, Loss: 4.374125957489014\n",
            "Epoch 36/100, Loss: 4.249030113220215\n",
            "Epoch 37/100, Loss: 4.124330520629883\n",
            "Epoch 38/100, Loss: 4.0056915283203125\n",
            "Epoch 39/100, Loss: 3.887681722640991\n",
            "Epoch 40/100, Loss: 3.7743523120880127\n",
            "Epoch 41/100, Loss: 3.66284441947937\n",
            "Epoch 42/100, Loss: 3.552198648452759\n",
            "Epoch 43/100, Loss: 3.4483370780944824\n",
            "Epoch 44/100, Loss: 3.3417344093322754\n",
            "Epoch 45/100, Loss: 3.241847276687622\n",
            "Epoch 46/100, Loss: 3.145601749420166\n",
            "Epoch 47/100, Loss: 3.0464253425598145\n",
            "Epoch 48/100, Loss: 2.955972194671631\n",
            "Epoch 49/100, Loss: 2.862358331680298\n",
            "Epoch 50/100, Loss: 2.7761471271514893\n",
            "Epoch 51/100, Loss: 2.6877121925354004\n",
            "Epoch 52/100, Loss: 2.6054539680480957\n",
            "Epoch 53/100, Loss: 2.521332025527954\n",
            "Epoch 54/100, Loss: 2.4417033195495605\n",
            "Epoch 55/100, Loss: 2.36519455909729\n",
            "Epoch 56/100, Loss: 2.289191246032715\n",
            "Epoch 57/100, Loss: 2.2159059047698975\n",
            "Epoch 58/100, Loss: 2.144134521484375\n",
            "Epoch 59/100, Loss: 2.074537754058838\n",
            "Epoch 60/100, Loss: 2.0066094398498535\n",
            "Epoch 61/100, Loss: 1.9404561519622803\n",
            "Epoch 62/100, Loss: 1.8766906261444092\n",
            "Epoch 63/100, Loss: 1.8143805265426636\n",
            "Epoch 64/100, Loss: 1.7534406185150146\n",
            "Epoch 65/100, Loss: 1.695032000541687\n",
            "Epoch 66/100, Loss: 1.6384356021881104\n",
            "Epoch 67/100, Loss: 1.5822309255599976\n",
            "Epoch 68/100, Loss: 1.5282591581344604\n",
            "Epoch 69/100, Loss: 1.4758992195129395\n",
            "Epoch 70/100, Loss: 1.425593614578247\n",
            "Epoch 71/100, Loss: 1.3758206367492676\n",
            "Epoch 72/100, Loss: 1.3284579515457153\n",
            "Epoch 73/100, Loss: 1.2813529968261719\n",
            "Epoch 74/100, Loss: 1.23609459400177\n",
            "Epoch 75/100, Loss: 1.1931601762771606\n",
            "Epoch 76/100, Loss: 1.1501917839050293\n",
            "Epoch 77/100, Loss: 1.108973741531372\n",
            "Epoch 78/100, Loss: 1.0694090127944946\n",
            "Epoch 79/100, Loss: 1.030179738998413\n",
            "Epoch 80/100, Loss: 0.9936827421188354\n",
            "Epoch 81/100, Loss: 0.9566253423690796\n",
            "Epoch 82/100, Loss: 0.9219548106193542\n",
            "Epoch 83/100, Loss: 0.8875272274017334\n",
            "Epoch 84/100, Loss: 0.8543621301651001\n",
            "Epoch 85/100, Loss: 0.8230891227722168\n",
            "Epoch 86/100, Loss: 0.791730523109436\n",
            "Epoch 87/100, Loss: 0.7620847821235657\n",
            "Epoch 88/100, Loss: 0.7329492568969727\n",
            "Epoch 89/100, Loss: 0.7049000263214111\n",
            "Epoch 90/100, Loss: 0.6779221892356873\n",
            "Epoch 91/100, Loss: 0.6519736051559448\n",
            "Epoch 92/100, Loss: 0.6263436675071716\n",
            "Epoch 93/100, Loss: 0.6021719574928284\n",
            "Epoch 94/100, Loss: 0.5783812999725342\n",
            "Epoch 95/100, Loss: 0.5557695627212524\n",
            "Epoch 96/100, Loss: 0.5336912870407104\n",
            "Epoch 97/100, Loss: 0.5123827457427979\n",
            "Epoch 98/100, Loss: 0.49177271127700806\n",
            "Epoch 99/100, Loss: 0.47214168310165405\n",
            "Epoch 100/100, Loss: 0.45278260111808777\n",
            "Continuing training with DoRA layers...\n",
            "Epoch 1/5, Loss: 0.4345872104167938\n",
            "Epoch 2/5, Loss: 0.41689610481262207\n",
            "Epoch 3/5, Loss: 0.3993534743785858\n",
            "Epoch 4/5, Loss: 0.383047878742218\n",
            "Epoch 5/5, Loss: 0.3669576346874237\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "tf.random.set_seed(0)\n",
        "\n",
        "class DoRALayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_in, d_out, rank=4, weight=None, bias=None, **kwargs):\n",
        "        super(DoRALayer, self).__init__(**kwargs)\n",
        "\n",
        "        self.d_in = d_in\n",
        "        self.d_out = d_out\n",
        "\n",
        "        if weight is not None:\n",
        "            self.weight = self.add_weight(shape=(d_out, d_in),\n",
        "                                          initializer=tf.constant_initializer(weight),\n",
        "                                          trainable=False)\n",
        "        else:\n",
        "            self.weight = self.add_weight(shape=(d_out, d_in),\n",
        "                                          initializer='random_normal',\n",
        "                                          trainable=False)\n",
        "\n",
        "        if bias is not None:\n",
        "            self.bias = self.add_weight(shape=(d_out,),\n",
        "                                        initializer=tf.constant_initializer(bias),\n",
        "                                        trainable=False)\n",
        "        else:\n",
        "            self.bias = self.add_weight(shape=(d_out,),\n",
        "                                        initializer='zeros',\n",
        "                                        trainable=False)\n",
        "\n",
        "        std_dev = 1 / tf.sqrt(tf.cast(rank, tf.float32))\n",
        "        self.lora_A = self.add_weight(shape=(d_out, rank),\n",
        "                                      initializer=tf.random_normal_initializer(stddev=std_dev),\n",
        "                                      trainable=True)\n",
        "        self.lora_B = self.add_weight(shape=(rank, d_in),\n",
        "                                      initializer='zeros',\n",
        "                                      trainable=True)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        lora = tf.matmul(self.lora_A, self.lora_B)\n",
        "        adapted = self.weight + lora\n",
        "        column_norm = tf.norm(adapted, ord=2, axis=0, keepdims=True)\n",
        "        norm_adapted = adapted / column_norm\n",
        "        m = tf.norm(self.weight, ord=2, axis=0, keepdims=True)\n",
        "        calc_weights = m * norm_adapted\n",
        "        return tf.add(tf.matmul(inputs, tf.transpose(calc_weights)), self.bias)\n",
        "\n",
        "class SimpleModel(tf.keras.Model):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(SimpleModel, self).__init__()\n",
        "        self.layer1 = tf.keras.layers.Dense(output_dim, input_dim=input_dim)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return self.layer1(inputs)\n",
        "\n",
        "def generate_data(num_samples=100, input_dim=10):\n",
        "    X = np.random.randn(num_samples, input_dim).astype(np.float32)\n",
        "    y = np.sum(X, axis=1, keepdims=True)\n",
        "    return X, y\n",
        "\n",
        "def count_model_params(model):\n",
        "    total_params = sum(np.prod(var.shape) for var in model.variables)\n",
        "    trainable_params = sum(np.prod(var.shape) for var in model.trainable_variables)\n",
        "    return total_params, trainable_params\n",
        "\n",
        "def print_model_params(model):\n",
        "    total_params, trainable_params = count_model_params(model)\n",
        "    print(f\"Total Parameters: {total_params}\")\n",
        "    print(f\"Trainable Parameters: {trainable_params}\")\n",
        "\n",
        "def train(model, optimizer, data, epochs=5):\n",
        "    #print_model_params(model)\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss_avg = tf.keras.metrics.Mean()\n",
        "        for step, (x_batch, y_batch) in enumerate(data):\n",
        "            with tf.GradientTape() as tape:\n",
        "                logits = model(x_batch, training=True)\n",
        "                loss_value = tf.reduce_mean(tf.keras.losses.MSE(y_batch, logits))\n",
        "            grads = tape.gradient(loss_value, model.trainable_weights)\n",
        "            optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "            epoch_loss_avg.update_state(loss_value)\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {epoch_loss_avg.result().numpy()}\")\n",
        "\n",
        "\n",
        "def replace_linear_with_dora(model, input_dim, output_dim):\n",
        "    for layer in model.layers:\n",
        "        if isinstance(layer, tf.keras.layers.Dense):\n",
        "            weights, biases = layer.get_weights()\n",
        "            new_layer = DoRALayer(input_dim, output_dim, weight=weights, bias=biases)\n",
        "            model.layers[model.layers.index(layer)] = new_layer\n",
        "    return model\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    input_dim, output_dim = 10, 1\n",
        "    model = SimpleModel(input_dim, output_dim)\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "    X, y = generate_data(num_samples=1000, input_dim=input_dim)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((X, y)).batch(64).shuffle(buffer_size=1000)\n",
        "\n",
        "    train(model, optimizer, dataset, epochs=100)\n",
        "\n",
        "\n",
        "    model = replace_linear_with_dora(model, input_dim, output_dim)\n",
        "\n",
        "    print(\"Continuing training with DoRA layers...\")\n",
        "    train(model, optimizer, dataset, epochs=5)\n"
      ]
    }
  ]
}