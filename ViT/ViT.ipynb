{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale: https://arxiv.org/pdf/2010.11929.pdf\n"
      ],
      "metadata": {
        "id": "EbPubfIRGSuV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "outputs = sum(values * pairwise_scores( query, keys ))"
      ],
      "metadata": {
        "id": "ju5fA0-FDfxL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "agbP_1H-4dXw"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "class MultiHeadSelfAttention(layers.Layer):\n",
        "    def __init__(self, num_heads, d_model):\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "        self.depth = d_model // self.num_heads\n",
        "        \n",
        "        self.query_dense = layers.Dense(units=d_model)\n",
        "        self.key_dense = layers.Dense(units=d_model)\n",
        "        self.value_dense = layers.Dense(units=d_model)\n",
        "        \n",
        "        self.dense = layers.Dense(units=d_model)\n",
        "\n",
        "    def attention(self, query, key, value):\n",
        "        # Calculate dot product attention\n",
        "        dot_product = tf.matmul(query, key, transpose_b=True)\n",
        "        dot_product = dot_product / tf.sqrt(tf.cast(self.depth, dtype=tf.float32))\n",
        "        weights = tf.nn.softmax(dot_product)\n",
        "\n",
        "        # Calculate weighted sum of values\n",
        "        output = tf.matmul(weights, value)\n",
        "        return output, weights\n",
        "        \n",
        "    def split_heads(self, inputs, batch_size):\n",
        "        inputs = tf.reshape(inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
        "    \n",
        "    def call(self, inputs):\n",
        "        # Linear layers\n",
        "        query = self.query_dense(inputs)\n",
        "        key = self.key_dense(inputs)\n",
        "        value = self.value_dense(inputs)\n",
        "        \n",
        "        # Split heads\n",
        "        batch_size = tf.shape(query)[0]\n",
        "        query = self.split_heads(query, batch_size)\n",
        "        key = self.split_heads(key, batch_size)\n",
        "        value = self.split_heads(value, batch_size)\n",
        "        \n",
        "        # Calculate dot product attention\n",
        "        output, weights = self.attention(query, key, value)\n",
        "        \n",
        "        # Concatenate heads\n",
        "        output = tf.transpose(output, perm=[0, 2, 1, 3])\n",
        "        output = tf.reshape(output, shape=(batch_size, -1, self.d_model))\n",
        "        \n",
        "        # Final linear layer\n",
        "        output = self.dense(output)\n",
        "        \n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = MultiHeadSelfAttention(num_heads, d_model)\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            layers.Dense(4*d_model, activation=\"relu\"),\n",
        "            layers.Dense(d_model)\n",
        "        ])\n",
        "        self.dropout1 = layers.Dropout(dropout)\n",
        "        self.dropout2 = layers.Dropout(dropout)\n",
        "        \n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)"
      ],
      "metadata": {
        "id": "Dk6ucyzZ6kLf"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The GELU Paper: https://arxiv.org/pdf/1606.08415v3.pdf"
      ],
      "metadata": {
        "id": "c6FnwG6fJXyN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " tf.math.erf computes the Gauss Error Function: https://www.tensorflow.org/api_docs/python/tf/math/erf\n",
        "\n",
        "https://www.wikiwand.com/en/Error_function\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AEyVB7DVu3y7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gelu(x):\n",
        "    cdf = 0.5 * (1.0 + tf.math.erf(x / tf.sqrt(2.0)))\n",
        "    #CDF stands for Cumulative Distribution Function\n",
        "    return x * cdf"
      ],
      "metadata": {
        "id": "x2ftRJQ5G-QE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model\n",
        "\n",
        "class VisionTransformer(Model):\n",
        "    def __init__(self, num_layers, d_model, num_heads, image_size, patch_size, dropout=0.1):\n",
        "        super(VisionTransformer, self).__init__()\n",
        "        self.image_size = image_size\n",
        "        self.patch_size = patch_size\n",
        "        self.encoder_layers = [TransformerBlock(d_model, num_heads, dropout) for _ in range(num_layers)]\n",
        "        self.layernorm = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.flatten = layers.Flatten()\n",
        "        self.dense = layers.Dense(units=64, activation=gelu)\n",
        "        self.classifier = layers.Dense(units=10, activation=\"softmax\")\n",
        "        \n",
        "    def extract_patches(self, images):\n",
        "        patches = tf.image.extract_patches(images, sizes=[1, self.patch_size, self.patch_size, 1], strides=[1, self.patch_size, self.patch_size, 1], rates=[1, 1, 1, 1], padding='VALID')\n",
        "        patches = tf.reshape(patches, shape=(-1, self.patch_size, self.patch_size, 1))\n",
        "        return patches\n",
        "    \n",
        "    def call(self, x, training):\n",
        "        x = tf.image.resize(x, size=[self.image_size, self.image_size])\n",
        "        x = self.extract_patches(x)\n",
        "        for layer in self.encoder_layers:\n",
        "            x = layer(x, training)\n",
        "        x = self.layernorm(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.dense(x)\n",
        "        return self.classifier(x)\n"
      ],
      "metadata": {
        "id": "i4zvC1fT86zV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_height=32\n",
        "patch_size=4\n",
        "num_layers=8  \n",
        "d_model=64 \n",
        "num_heads=4"
      ],
      "metadata": {
        "id": "Y7SmKEZnAMHg"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = VisionTransformer(\n",
        "            image_size=img_height,\n",
        "            patch_size= patch_size,\n",
        "            num_layers= num_layers,\n",
        "            d_model=d_model,\n",
        "            num_heads=num_heads,\n",
        "            dropout=0.1,\n",
        "        )"
      ],
      "metadata": {
        "id": "VnHh7NgxAFkR"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "            optimizer=tf.optimizers.Adam(learning_rate=0.01),\n",
        "            metrics=[\"accuracy\"],\n",
        "        )"
      ],
      "metadata": {
        "id": "RzcO7r1bByYy"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4LyPRqRrBtSD",
        "outputId": "58d75c84-86cd-408b-f323-be39a46d5327"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method Model.summary of <__main__.VisionTransformer object at 0x7feefd5a0700>>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    }
  ]
}