{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Distilling the Knowledge in a Neural Network: https://arxiv.org/pdf/1503.02531.pdf"
      ],
      "metadata": {
        "id": "fH4InORqLhzB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sYntO4yK_gFq"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# define the model architecture for the student model\n",
        "def create_student_model(input_shape=(32, 32, 3), num_classes=10):\n",
        "  inputs = keras.Input(shape=input_shape)\n",
        "  x = layers.Rescaling(1./255)(inputs)\n",
        "  x = layers.Conv2D(filters=64, kernel_size=3)(x)\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  x = layers.Activation(\"relu\")(x)\n",
        "  x = layers.Flatten(name='flatten')(x)\n",
        "  x = layers.Dense(units=16, activation = \"relu\")(x)\n",
        "  outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
        "  student_model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "  return student_model\n",
        "\n",
        "# define the model architecture for the teacher model\n",
        "def create_teacher_model(input_shape=(32, 32, 3), num_classes=10):\n",
        "  inputs = keras.Input(shape=input_shape)\n",
        "  x = layers.Rescaling(1./255)(inputs)\n",
        "  x = layers.Conv2D(filters=64, kernel_size=3)(x)\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  x = layers.Activation(\"relu\")(x)\n",
        "  x = layers.Conv2D(filters=32, kernel_size=3)(x)\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  x = layers.Activation(\"relu\")(x)\n",
        "  x = layers.Conv2D(filters=16, kernel_size=3)(x)\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  x = layers.Activation(\"relu\")(x)\n",
        "  x = layers.Flatten(name='flatten')(x)\n",
        "  x = layers.Dense(units=16, activation = \"relu\")(x)\n",
        "  outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
        "  teacher_model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "  return teacher_model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#This code was taken from: https://keras.io/examples/vision/knowledge_distillation/\n",
        "class Distiller(keras.Model):\n",
        "    def __init__(self, student, teacher):\n",
        "        super().__init__()\n",
        "        self.teacher = teacher\n",
        "        self.student = student\n",
        "\n",
        "    def compile(\n",
        "        self,\n",
        "        optimizer,\n",
        "        metrics,\n",
        "        student_loss_fn,\n",
        "        distillation_loss_fn,\n",
        "        alpha=0.1,\n",
        "        temperature=3,\n",
        "    ):\n",
        "        \"\"\" Configure the distiller.\n",
        "\n",
        "        Args:\n",
        "            optimizer: Keras optimizer for the student weights\n",
        "            metrics: Keras metrics for evaluation\n",
        "            student_loss_fn: Loss function of difference between student\n",
        "                predictions and ground-truth\n",
        "            distillation_loss_fn: Loss function of difference between soft\n",
        "                student predictions and soft teacher predictions\n",
        "            alpha: weight to student_loss_fn and 1-alpha to distillation_loss_fn\n",
        "            temperature: Temperature for softening probability distributions.\n",
        "                Larger temperature gives softer distributions.\n",
        "        \"\"\"\n",
        "        super().compile(optimizer=optimizer, metrics=metrics)\n",
        "        self.student_loss_fn = student_loss_fn\n",
        "        self.distillation_loss_fn = distillation_loss_fn\n",
        "        self.alpha = alpha\n",
        "        self.temperature = temperature\n",
        "\n",
        "    @tf.function\n",
        "    def train_step(self, data):\n",
        "        # Unpack data\n",
        "        x, y = data\n",
        "\n",
        "        # Forward pass of teacher\n",
        "        teacher_predictions = self.teacher(x, training=False)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Forward pass of student\n",
        "            student_predictions = self.student(x, training=True)\n",
        "\n",
        "            # Compute losses\n",
        "            student_loss = self.student_loss_fn(y, student_predictions)\n",
        "\n",
        "            # Compute scaled distillation loss from https://arxiv.org/abs/1503.02531\n",
        "            # The magnitudes of the gradients produced by the soft targets scale\n",
        "            # as 1/T^2, multiply them by T^2 when using both hard and soft targets.\n",
        "            distillation_loss = (\n",
        "                self.distillation_loss_fn(\n",
        "                    tf.nn.softmax(teacher_predictions / self.temperature, axis=1),\n",
        "                    tf.nn.softmax(student_predictions / self.temperature, axis=1),\n",
        "                )\n",
        "                * self.temperature**2\n",
        "            )\n",
        "\n",
        "            loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss\n",
        "\n",
        "        # Compute gradients\n",
        "        trainable_vars = self.student.trainable_variables\n",
        "        gradients = tape.gradient(loss, trainable_vars)\n",
        "\n",
        "        # Update weights\n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "\n",
        "        # Update the metrics configured in `compile()`.\n",
        "        self.compiled_metrics.update_state(y, student_predictions)\n",
        "\n",
        "        # Return a dict of performance\n",
        "        results = {m.name: m.result() for m in self.metrics}\n",
        "        results.update(\n",
        "            {\"student_loss\": student_loss, \"distillation_loss\": distillation_loss}\n",
        "        )\n",
        "        return results\n",
        "\n",
        "    @tf.function\n",
        "    def test_step(self, data):\n",
        "        # Unpack the data\n",
        "        x, y = data\n",
        "\n",
        "        # Compute predictions\n",
        "        y_prediction = self.student(x, training=False)\n",
        "\n",
        "        # Calculate the loss\n",
        "        student_loss = self.student_loss_fn(y, y_prediction)\n",
        "\n",
        "        # Update the metrics.\n",
        "        self.compiled_metrics.update_state(y, y_prediction)\n",
        "\n",
        "        # Return a dict of performance\n",
        "        results = {m.name: m.result() for m in self.metrics}\n",
        "        results.update({\"student_loss\": student_loss})\n",
        "        return results"
      ],
      "metadata": {
        "id": "PFYtzuwGBp2s"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "student_model = create_student_model()\n",
        "teacher_model = create_teacher_model()"
      ],
      "metadata": {
        "id": "s3h2_9KKCxdt"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()"
      ],
      "metadata": {
        "id": "h7rpUw2-CkNX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "teacher_model.compile(\n",
        "    optimizer=keras.optimizers.Adam(),\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        ")"
      ],
      "metadata": {
        "id": "iZoReVa_DcsZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "teacher_model.fit(x_train, y_train, epochs=40)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7j4UEGadDonp",
        "outputId": "8865d3bf-96bf-4d6a-cdca-d1a0b71c53ab"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1563/1563 [==============================] - 11s 5ms/step - loss: 1.8723 - sparse_categorical_accuracy: 0.2839\n",
            "Epoch 2/40\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.6503 - sparse_categorical_accuracy: 0.3537\n",
            "Epoch 3/40\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.5666 - sparse_categorical_accuracy: 0.3828\n",
            "Epoch 4/40\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.5017 - sparse_categorical_accuracy: 0.4109\n",
            "Epoch 5/40\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 1.4621 - sparse_categorical_accuracy: 0.4271\n",
            "Epoch 6/40\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.4239 - sparse_categorical_accuracy: 0.4412\n",
            "Epoch 7/40\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.3921 - sparse_categorical_accuracy: 0.4564\n",
            "Epoch 8/40\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.3603 - sparse_categorical_accuracy: 0.4709\n",
            "Epoch 9/40\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.3361 - sparse_categorical_accuracy: 0.4799\n",
            "Epoch 10/40\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.2717 - sparse_categorical_accuracy: 0.5217\n",
            "Epoch 11/40\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1625 - sparse_categorical_accuracy: 0.5649\n",
            "Epoch 12/40\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1065 - sparse_categorical_accuracy: 0.5887\n",
            "Epoch 13/40\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.0617 - sparse_categorical_accuracy: 0.6059\n",
            "Epoch 14/40\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.0335 - sparse_categorical_accuracy: 0.6163\n",
            "Epoch 15/40\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.0012 - sparse_categorical_accuracy: 0.6279\n",
            "Epoch 16/40\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.9668 - sparse_categorical_accuracy: 0.6396\n",
            "Epoch 17/40\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.8977 - sparse_categorical_accuracy: 0.6688\n",
            "Epoch 18/40\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.8359 - sparse_categorical_accuracy: 0.6923\n",
            "Epoch 19/40\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.8017 - sparse_categorical_accuracy: 0.7052\n",
            "Epoch 20/40\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.7665 - sparse_categorical_accuracy: 0.7170\n",
            "Epoch 21/40\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.7381 - sparse_categorical_accuracy: 0.7303\n",
            "Epoch 22/40\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.7062 - sparse_categorical_accuracy: 0.7399\n",
            "Epoch 23/40\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6889 - sparse_categorical_accuracy: 0.7460\n",
            "Epoch 24/40\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6601 - sparse_categorical_accuracy: 0.7558\n",
            "Epoch 25/40\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6405 - sparse_categorical_accuracy: 0.7639\n",
            "Epoch 26/40\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.6221 - sparse_categorical_accuracy: 0.7687\n",
            "Epoch 27/40\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6018 - sparse_categorical_accuracy: 0.7756\n",
            "Epoch 28/40\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.5885 - sparse_categorical_accuracy: 0.7791\n",
            "Epoch 29/40\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.5698 - sparse_categorical_accuracy: 0.7880\n",
            "Epoch 30/40\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.5557 - sparse_categorical_accuracy: 0.7942\n",
            "Epoch 31/40\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.5378 - sparse_categorical_accuracy: 0.8000\n",
            "Epoch 32/40\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.5261 - sparse_categorical_accuracy: 0.8043\n",
            "Epoch 33/40\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.5080 - sparse_categorical_accuracy: 0.8096\n",
            "Epoch 34/40\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.4988 - sparse_categorical_accuracy: 0.8129\n",
            "Epoch 35/40\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.4879 - sparse_categorical_accuracy: 0.8186\n",
            "Epoch 36/40\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.4712 - sparse_categorical_accuracy: 0.8246\n",
            "Epoch 37/40\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.4579 - sparse_categorical_accuracy: 0.8294\n",
            "Epoch 38/40\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.4485 - sparse_categorical_accuracy: 0.8330\n",
            "Epoch 39/40\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.4325 - sparse_categorical_accuracy: 0.8374\n",
            "Epoch 40/40\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.4213 - sparse_categorical_accuracy: 0.8440\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f60d665dc10>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "teacher_model.evaluate(x_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1uZTP4NgCxnR",
        "outputId": "45c67b14-49d3-48bb-df69-bbb3a9ba51a9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 4ms/step - loss: 2.1459 - sparse_categorical_accuracy: 0.5146\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.1458559036254883, 0.5145999789237976]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "distiller = Distiller(student=student_model, teacher=teacher_model)"
      ],
      "metadata": {
        "id": "bqO8VeINF7HD"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras.backend as K\n",
        "from keras.losses import Loss\n",
        "\n",
        "class KLDivergence(Loss):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(KLDivergence, self).__init__(*args, **kwargs)\n",
        "        \n",
        "    def call(self, y_true, y_pred):\n",
        "        y_true = K.clip(y_true, K.epsilon(), 1)\n",
        "        y_pred = K.clip(y_pred, K.epsilon(), 1)\n",
        "        return K.sum(y_true * K.log(y_true / y_pred), axis=-1)"
      ],
      "metadata": {
        "id": "M8Z1xj9eJrsm"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "distiller.compile(\n",
        "    optimizer=keras.optimizers.Adam(),\n",
        "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        "    student_loss_fn=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    distillation_loss_fn=KLDivergence(),\n",
        "    alpha=0.1,\n",
        "    temperature=10,\n",
        ")"
      ],
      "metadata": {
        "id": "LF2JnQ7bGBka"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "distiller.fit(x_train, y_train, epochs=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mek95r-vGEWE",
        "outputId": "c56474f7-ab93-4858-e93b-e67f9f601a1e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1563/1563 [==============================] - 9s 5ms/step - sparse_categorical_accuracy: 0.0982 - student_loss: 2.3143 - distillation_loss: 0.0330\n",
            "Epoch 2/5\n",
            "1563/1563 [==============================] - 8s 5ms/step - sparse_categorical_accuracy: 0.0982 - student_loss: 2.3030 - distillation_loss: 0.0329\n",
            "Epoch 3/5\n",
            "1563/1563 [==============================] - 8s 5ms/step - sparse_categorical_accuracy: 0.0995 - student_loss: 2.3030 - distillation_loss: 0.0329\n",
            "Epoch 4/5\n",
            "1563/1563 [==============================] - 8s 5ms/step - sparse_categorical_accuracy: 0.0997 - student_loss: 2.3031 - distillation_loss: 0.0329\n",
            "Epoch 5/5\n",
            "1563/1563 [==============================] - 8s 5ms/step - sparse_categorical_accuracy: 0.0991 - student_loss: 2.3031 - distillation_loss: 0.0329\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f60d63a5eb0>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "distiller.evaluate(x_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eD23vpj3F5f3",
        "outputId": "2f897a93-fd2b-42b3-bac6-6af50dea7263"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 3ms/step - sparse_categorical_accuracy: 0.1000 - student_loss: 2.3029\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.10000000149011612, 2.316351890563965]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    }
  ]
}